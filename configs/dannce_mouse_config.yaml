### basic ###
# File name of io.yaml file, which should exist in the directory from which you call dannce functions
io_config: io.yaml
random_seed: 1024

### data ###
dataset: label3d # dataset type, options are 'label3d', 'rat7m', 'pair'
camnames: ['Camera1', 'Camera2', 'Camera3', 'Camera4', 'Camera5', 'Camera6']
n_views: 6
num_validation_per_exp: 4
data_split_seed: 1024

vmin: -60 # lower and upper bound of the 3D volume (in mm) anchored on animal
vmax: 60
# OR, use vol_size: 120 to replace the above two lines
nvox: 80
interp: nearest

expval: True # if False, will use a 3D Gaussian target instead

### data augmentations ###      
medfilt_window: 30
rand_view_replace: True
n_rand_views: 6
mirror_augmentation: False
augment_hue: False
augment_brightness: False
augment_bright_val: 0.01

### model ###
# architecture options:
# "dannce"
# "compressed_dannce" (a channel-compressed version of the original dannce encoder-decoder)
net_type: "compressed_dannce" 
n_channels_in: 3
n_channels_out: 22 # number of output channels

### train ###
batch_size: 4
epochs: 1200
save_period: 100

# Options:
# 'new': initializes and trains a network from scratch
# 'finetune': loads in pre-trained weights and fine-tuned from there
# 'continued': initializes a full model, including optimizer state, and continuous training from the last full model checkpoint
train_mode: new
dannce_finetune_weights: ./DANNCE/weights/dannce-c-r7m-demo.pth # needed for 'finetune' and 'continued' mode

loss: 
  L1Loss:
    loss_weight: 1.0

metric: ['euclidean_distance_3D']

lr: 0.0001
# one may find the available learning schedulers in 
# https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
#lr_scheduler:
#  type: StepLR
#  args: 
#    step_size: 100
#    gamma: 0.1

### prediction ###
max_num_samples: 1000
